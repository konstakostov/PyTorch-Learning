{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Import needed library  \n",
    "torch.nn -> Parent object for PyTorch models  \n",
    "torch.nn.functional -> Activation function"
   ],
   "id": "fd30be49ec027af7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-25T10:27:44.393456Z",
     "start_time": "2024-06-25T10:27:42.566407Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LeNet-5\n",
    "![title](images/image_02.png)\n",
    "\n",
    "One of the earliest CNN's. It was build to read small images of handwritten numbers and correctly classify which digit is represented in the image.\n",
    "\n",
    "Here’s the abridged version of how it works:\n",
    "- Layer C1 is a convolutional layer, meaning that it scans the input image for features it learned during training. It outputs a map of where it saw each of its learned features in the image. This “activation map” is downsampled in layer S2.\n",
    "- Layer C3 is another convolutional layer, this time scanning C1’s activation map for combinations of features. It also puts out an activation map describing the spatial locations of these feature combinations, which is downsampled in layer S4.\n",
    "- Finally, the fully-connected layers at the end, F5, F6, and OUTPUT, are a classifier that takes the final activation map, and classifies it into one of ten bins representing the 10 digits.\n",
    "\n",
    "This CNN is represented with the following code:"
   ],
   "id": "561816d62ca55371"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T10:27:44.403167Z",
     "start_time": "2024-06-25T10:27:44.395551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # 1 input image channel (B&W), 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # An affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)   # 5 * 5 from image dimensions\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # If the size is square only a single number can be specified\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        # All dimensions except the batch dimensions\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        \n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        \n",
    "        return num_features"
   ],
   "id": "d52c2389ec293396",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T10:27:44.421370Z",
     "start_time": "2024-06-25T10:27:44.404471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = LeNet()\n",
    "print(net)\n",
    "\n",
    "# Stand-in for 32x32 B&W image\n",
    "input_stand_in_image = torch.rand(1, 1, 32, 32)\n",
    "\n",
    "print(f\"\\nImage batch shape:\\n{input_stand_in_image.shape}\")\n",
    "\n",
    "# Output\n",
    "# forward() is not called directly\n",
    "output = net(input_stand_in_image)\n",
    "print(f\"\\nRaw output:\\n{output}\")\n",
    "print(output.shape)"
   ],
   "id": "d217ed3763a4b260",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[-0.1030,  0.0809,  0.1457,  0.1024,  0.1265, -0.0276,  0.0549,  0.0919,\n",
      "          0.0300,  0.0299]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T10:27:44.433625Z",
     "start_time": "2024-06-25T10:27:44.425281Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "95ea00e1d51e65f4",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
