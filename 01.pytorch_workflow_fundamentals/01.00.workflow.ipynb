{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch Workflow Fundamentals\n",
    "### (One of many)\n",
    "\n",
    "1. Get data ready (turn it into tensors)\n",
    "2. Build or pick a pretrained model (suiting your problem)\n",
    "    * Pick a loss function & optimizer\n",
    "    * Build a training loop\n",
    "    * Repeat step 2 until necessary.\n",
    "3. Fit the model to the data and make a prediction\n",
    "4. Evaluate the model\n",
    "5. Improve through experimentation\n",
    "6. Save and reload your trained model\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Topic                                                    | Contents                                                                                                                          |\n",
    "|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1. Getting data ready                                    | Data can be almost anything but to get started we're going to create a simple straight line                                       |\n",
    "| 2. Building a model                                      | Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer, and build a training loop. |\n",
    "| 3. Fitting the model to data (training)                  | We've got data and a model, now let's let the model (try to) find patterns in the (training) data.                                |\n",
    "| 4. Making predictions and evaluating a model (inference) | Our model's found patterns in the data, let's compare its findings to the actual (testing) data.                                  |\n",
    "| 5. Saving and loading a model                            | You may want to use your model elsewhere, or come back to it later, here we'll cover that.                                        |\n",
    "| 6. Putting it all together                               | Let's take all of the above and combine it.                                                                                       |\n",
    "<br>\n",
    "\n",
    "<p>\n",
    "    <img src=\"01.markdown_images/01_a_pytorch_workflow.png\" alt=\"A PyTorch Workflow\" width=\"800\" height=\"440\">\n",
    "</p>"
   ],
   "id": "962c42ac81a45290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing the necessary libraries",
   "id": "a62d6673b96a7871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn    # All of PyTorch's building blocks for Neural Networks\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "435bfd71fea16e80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data (Preparing and Loading)\n",
    "\n",
    "Data can be almost anything (in ML):\n",
    "* Excel spreadsheet\n",
    "* Image(s)\n",
    "* Video(s)\n",
    "* Audio\n",
    "* DNA\n",
    "* Text\n",
    "* etc.\n",
    "\n",
    "Machine Learning is a game of two parts:\n",
    "1. Get the data into numerical representation.\n",
    "2. Build a model to learn patters in that numerical representation. \n",
    "\n",
    "<p>\n",
    "    <img src=\"01.markdown_images/01-machine-learning-a-game-of-two-parts.png\" alt=\"ML - A Game in Two Parts\" width=\"800\" height=\"440\">\n",
    "</p>"
   ],
   "id": "7b83c2e9dc6345ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Linear Regression\n",
    "\n",
    "Using linear regression formula to make a straight line with *known* **parameters**.\n",
    "\n",
    "#### Y = a + bX\n",
    "\n",
    "where:\n",
    "- **Y** is the dependent variable (*the outcome or response variable*).\n",
    "- **X** is the independent variable (*the predictor or explanatory variable*).\n",
    "- **a** is the intercept of the regression line (*the value of **Y** when **X** is 0*).\n",
    "- **b** is the slope of the regression line (*the change in **Y** for a one-unit change in **X***)."
   ],
   "id": "c3a9e41de9c2ff12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating known parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# .unsqueeze(dim=1) Adds 1 extra dimension\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "\n",
    "X[:10], len(X), y[:10],len(y)"
   ],
   "id": "eb319b592d2bb72d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Splitting data int Training & Test sets\n",
    "\n",
    "There are three types of datasets:\n",
    "* ***Training set*** -> The model learns patterns from here\n",
    "* ***Validation set*** -> Tune model patterns\n",
    "* ***Test set*** -> See if the model is ready\n",
    "\n",
    "| Split          | Purpose                                                                                                                      | Amount of total data | How often is it used?    |\n",
    "|----------------|------------------------------------------------------------------------------------------------------------------------------|----------------------|--------------------------|\n",
    "| Training set   | The model learns from this data (like the course materials you study during the semester).                                   | ~60-80%              | Always                   |\n",
    "| Validation set | The model gets tuned on this data (like the practice exam you take before the final exam).                                   | ~10-20%              | Often but not always     |\n",
    "| Testing set    | The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). | ~10-20%              | Always                   |\n",
    "\n",
    "\n",
    "**Generalization**: The ability for a machine learning model to perform well on data it hasn't seen."
   ],
   "id": "1f64414b068f71ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating a train/test split\n",
    "train_split = int(0.8 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ],
   "id": "2a17c5980225b27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Printing the training sets\n",
    "X_train, y_train"
   ],
   "id": "29cb65580140e22d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## \"Visualize, visualize, visualize!\"\n",
    "\n",
    "Visualising our data for better understanding of it."
   ],
   "id": "61690e194f3ebd80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_predictions(train_data,\n",
    "                     train_labels,\n",
    "                     test_data,\n",
    "                     test_labels,\n",
    "                     predictions,\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    PLots training data, test data and compares predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating a plot figure\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Plotting the training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "    \n",
    "    # Plotting the training data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "    \n",
    "    # Check if there are predictions\n",
    "    if predictions is not None:\n",
    "        # Plot predictions if they exist\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "    \n",
    "    # Show the legend\n",
    "    plt.legend(prop={\"size\": 12})"
   ],
   "id": "1981756d21fd6294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_predictions(train_data=X_train,\n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=None)"
   ],
   "id": "211cdb2fc9eda77e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building our first model\n",
    "\n",
    "The first model will be using Linear Regression. \n",
    "\n",
    "What this model does: \n",
    "* Start with random values (weight & bias)\n",
    "* Look at the training data and adjust the random values to better represent (*or get closer to*) the ideal values (*the weight & bias values we used to create the data*).\n",
    "\n",
    "This will be done via two main algorithms:\n",
    "1. Gradient descent\n",
    "    * [3Blue1Brown](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "    * [towardsdatascience.com](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c)\n",
    "2. Backpropagation\n",
    "    * [3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "    * [towardsdatascience.com](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)"
   ],
   "id": "4197463c1b14223e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PyTorch model building essentials\n",
    "\n",
    "| PyTorch module        | What does it do?                                                                                                                                                                                                                                  |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `torch.nn`            | Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way).                                                                                                                 |\n",
    "| `torch.nn.Parameter`  | Stores tensors that can be used with `nn.Module`. If `requires_grad=True`, gradients (used for updating model parameters via gradient descent) are calculated automatically; this is often referred to as \"autograd\".                             |\n",
    "| `torch.nn.Module`     | The base class for all neural network modules; all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass `nn.Module`. Requires a `forward()` method be implemented. |\n",
    "| `torch.optim`         | Contains various optimization algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss).                                                                   |\n",
    "| `def forward()`       | All `nn.Module` subclasses require a `forward()` method; this defines the computation that will take place on the data passed to the particular `nn.Module` (e.g., the linear regression formula above).                                          |\n",
    "\n",
    "* [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) -> contains the larger building blocks (layers)\n",
    "* [`nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) -> contains the smaller parameters like weights and biases (put these together to make nn.Module(s))\n",
    "* `forward()` -> tells the larger blocks how to make calculations on inputs (tensors full of data) within nn.Module(s)\n",
    "* [`torch.optim`](https://pytorch.org/docs/stable/optim.html) -> contains optimization methods on how to improve the parameters within nn.Parameter to better represent input data\n",
    "\n",
    "\n",
    "#### [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html)"
   ],
   "id": "9d41e546165843d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Weight parameter\n",
    "        self.weights = nn.Parameter(torch.randn(1, \n",
    "                                                requires_grad=True, # Setting Requirement for Gradient Descent as True\n",
    "                                                dtype=torch.float)) # Setting default dtype float32\n",
    "        \n",
    "        # Bias parameter\n",
    "        self.bias = nn.Parameter(torch.randn(1,\n",
    "                                             requires_grad=True, # Setting Requirement for Gradient Descent as True\n",
    "                                             dtype=torch.float)) # Setting default dtype float32\n",
    "    \n",
    "    # Forward method defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:     # \"x\" is the input data\n",
    "        return self.weights * x + self.bias  # Linear Regression formula; \"x\" is the input data"
   ],
   "id": "892066f5b6980c26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Basic building blocks of creating a PyTorch model by subclassing nn.Module. For objects that subclass nn.Module, the forward() method must be defined.\n",
    "\n",
    "<p>\n",
    "    <img src=\"01.markdown_images/01-pytorch-linear-model-annotated.png\" alt=\"PyTorch Linear Model Annotated\" width=\"800\" height=\"440\">\n",
    "</p>"
   ],
   "id": "58c3931944709c2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Checking the contents of our fist created model\n",
    "\n",
    "The contents of the model can be viewed with `.parameters`"
   ],
   "id": "dd9e8bb859c7e7cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating a random seed \n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Creating an instance of the model (this is a subclass of `nn.Module`)\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Checking our model's parameters (List unnamed parameters)\n",
    "list(model_0.parameters())"
   ],
   "id": "eca97a894d535d7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List named parameters\n",
    "print(model_0.state_dict())"
   ],
   "id": "9034c4fa9c8c306f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Making predictions using [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.inference_mode.html)\n",
    "\n",
    "To check our model's predictive power, we test how well it redicts `y_test` based on `X_test`.\n",
    "\n",
    "When we pass data through our model, it will run it through our `forward()` method."
   ],
   "id": "95b6aa5b1e3aca25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make predictions with a model\n",
    "with torch.inference_mode():\n",
    "    y_predictions = model_0(X_test)"
   ],
   "id": "e9668305414262cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Results for y_predictions\n",
    "y_predictions"
   ],
   "id": "794bc7bf2048f33c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Results for y_test\n",
    "y_test"
   ],
   "id": "f305dd9d6699b301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualising the data\n",
    "plot_predictions(train_data=X_train,\n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=y_predictions)\n",
    "\n",
    "# Because we are using random data, our predictions are way off.\n",
    "# The Predictions (red) should be closer to the testing data (green)."
   ],
   "id": "158728c5194f6193",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a model\n",
    "\n",
    "The whole idea of training is for a model to move from some *unknown* parameters (these may be random) to some *known* parameters. \n",
    "\n",
    "In other words: From a poor representation of the data to a better representation of data. \n",
    "\n",
    "One way to measure how poor or how wrong your model's predictions are is to use ***loss function***.\n",
    "\n",
    "***`Loss Function`*** === ***`Cost Function`*** === ***`Criterion`***. These names are for the same thing in different areas. \n",
    "\n",
    "<br>\n",
    "\n",
    "| Function             | What does it do?                                                                                                                  | Where does it live in PyTorch?                                               | Common values                                                                                                                                             |\n",
    "|----------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Loss function        | Measures how wrong your model's predictions (e.g., `y_preds`) are compared to the truth labels (e.g., `y_test`). Lower is better. | PyTorch has plenty of built-in loss functions in `torch.nn`.                 | Mean absolute  error (MAE) for regression problems (`torch.nn.L1Loss()`). Binary cross entropy for binary classification problems (`torch.nn.BCELoss()`). |\n",
    "| Optimizer            | Tells your model how to update its internal parameters to best lower the loss.                                                    | You can find various optimization function implementations in `torch.optim`. | Stochastic gradient descent (`torch.optim.SGD()`). Adam optimizer (`torch.optim.Adam()`).                                                                 |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Things we need to train: \n",
    "* **Loss Function**: A function to measure how wrong your model's predictions are to the ideal outputs. **Lower** is ***better***.\n",
    "    * [PyTorch Documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "* **Optimizer**: Takes into account the loss of a model and adjusts the model's parameters (e.g., weight & bias) to improve the loss function.\n",
    "    * [PyTorch Documentation](https://pytorch.org/docs/stable/optim.html)\n",
    "    * In the optimizer you'' often have to set two parameters:\n",
    "        * `params=`:  The model parameters you'd ike to optimize, for example `params=model_0.parameters()`\n",
    "        * `lr` (Learning rate): The learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (`Small LR` --> Small changes, `Big LR` -> Large changes)\n",
    "\n",
    "\n",
    "#### Things we need specifically for PyTorch.\n",
    "* Training Loop:\n",
    "* Testing Loop:  \n"
   ],
   "id": "e7b3a163b4056ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(model_0.parameters())",
   "id": "d5b0387c127a3c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Checking our model's parameters.\n",
    "# Parameter is a value that the model sets itself.\n",
    "model_0.state_dict()"
   ],
   "id": "25bc1095bff70306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setting up a loss functions\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Setting up and optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
    "                            lr=0.01,)   # lr is possibly the most important hyperparameter you can set. "
   ],
   "id": "e96ffbf6fedd56fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building a training loop (and testing loop) in PyTorch\n",
    "\n",
    "Things we need in a training loop: \n",
    "0. Loop through the data and do: \n",
    "    1. Forward Pass/Forward Propagation: Involves data moving from through our model's `forward()` functions to make predictions on data;\n",
    "    2. Calculate the Loss: Compare forward pass predictions to ground truth labels;\n",
    "    3. Optimizer zero grad;\n",
    "    4. Loss backward: Move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (***backpropagation***);\n",
    "    5. Optimizer step: Use the optimizer to adjust our model's parameters to try and improve the loss (***gradient descent***).\n",
    "\n",
    "### [Unofficial PyTorch Optimization Loop Song](https://www.youtube.com/watch?v=Nutpusq_AFw)\n",
    "It's train time!<br>\n",
    "Do the forward pass,<br>\n",
    "Calculate the loss,<br>\n",
    "Optimizer zero grad,<br>\n",
    "Loss backwards!<br>\n",
    "<br>\n",
    "Optimizer step, step, step.<br>\n",
    "<br>\n",
    "Let's test now!<br>\n",
    "With torch no grad:<br>\n",
    "Do the forward pass,<br>\n",
    "Calculate the loss,<br>\n",
    "Watch it go down down down!<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>\n",
    "    <img src=\"01.markdown_images/01-pytorch-training-loop-annotated.png\" alt=\"PyTorch Training Loop Annotated\" width=\"800\" height=\"440\">\n",
    "</p>"
   ],
   "id": "92f0a0b4419fa920"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Epoch is one loop through the data (hyperparameter, because we've set it ourselves).\n",
    "epochs = 1\n",
    "\n",
    "# # Training\n",
    "# 0. Loop through the data: \n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    # Train mode in PyTorch sets all parameters that require gradients to require gradients\n",
    "    model_0.train()\n",
    "    \n",
    "    # 1. Forward Pass\n",
    "    y_pred = model_0(X_train)\n",
    "    \n",
    "    # 2. Calculating the loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Loss backward/Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    # By default how the optimizer changes will accumulate through the loop, so they have to be zeroed in step 3 (Optimizer zero grad) for the next iteration of the loop\n",
    "    optimizer.step()\n",
    "    \n",
    "    # # Turns off gradient tracking\n",
    "    # model_0.eval()"
   ],
   "id": "9cef8e5da861117e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e63ccc3e77e80be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "897d558004473abc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
